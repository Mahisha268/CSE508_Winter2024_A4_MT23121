{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8163326,"sourceType":"datasetVersion","datasetId":4830074},{"sourceId":8171883,"sourceType":"datasetVersion","datasetId":4836510}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'data.csv' is your original dataset file\ndata = pd.read_csv('/kaggle/input/amazon/Reviews.csv')\n\n# Define the number of rows to sample\nsample_size = 5000\n\n# Assuming you want to sample the first 100,000 rows\nsampled_data = data[:sample_size]\n\n# Save the sampled data to a new CSV file\nsampled_data.to_csv('sampled_amazon_data.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:40:19.309469Z","iopub.execute_input":"2024-04-23T04:40:19.309813Z","iopub.status.idle":"2024-04-23T04:40:27.703181Z","shell.execute_reply.started":"2024-04-23T04:40:19.309784Z","shell.execute_reply":"2024-04-23T04:40:27.702357Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"sampled_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:40:49.533039Z","iopub.execute_input":"2024-04-23T04:40:49.533403Z","iopub.status.idle":"2024-04-23T04:40:49.560071Z","shell.execute_reply.started":"2024-04-23T04:40:49.533372Z","shell.execute_reply":"2024-04-23T04:40:49.559101Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Id   ProductId          UserId                      ProfileName  \\\n0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n\n   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n0                     1                       1      5  1303862400   \n1                     0                       0      1  1346976000   \n2                     1                       1      4  1219017600   \n3                     3                       3      2  1307923200   \n4                     0                       0      5  1350777600   \n\n                 Summary                                               Text  \n0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n2  \"Delight\" says it all  This is a confection that has been around a fe...  \n3         Cough Medicine  If you are looking for the secret ingredient i...  \n4            Great taffy  Great taffy at a great price.  There was a wid...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>ProfileName</th>\n      <th>HelpfulnessNumerator</th>\n      <th>HelpfulnessDenominator</th>\n      <th>Score</th>\n      <th>Time</th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>delmartian</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1303862400</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>dll pa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1346976000</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>B000LQOCH0</td>\n      <td>ABXLMWJIXXAIN</td>\n      <td>Natalia Corres \"Natalia Corres\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1219017600</td>\n      <td>\"Delight\" says it all</td>\n      <td>This is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>B000UA0QIQ</td>\n      <td>A395BORC6FGVXV</td>\n      <td>Karl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1307923200</td>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>B006K2ZZ7K</td>\n      <td>A1UQRSCLF8GW1T</td>\n      <td>Michael D. Bigham \"M. Wassir\"</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1350777600</td>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'data.csv' is your original dataset file\ndata = pd.read_csv('/kaggle/working/sampled_amazon_data.csv')\n\n# Keep only the 'Text' and 'Summary' columns\ndata_subset = data[['Summary', 'Text']]\n\n# Save the subset data to a new CSV file\ndata_subset.to_csv('amazon_text_summary_subset.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:41:00.122450Z","iopub.execute_input":"2024-04-23T04:41:00.122794Z","iopub.status.idle":"2024-04-23T04:41:00.269797Z","shell.execute_reply.started":"2024-04-23T04:41:00.122766Z","shell.execute_reply":"2024-04-23T04:41:00.268944Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_subset.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:41:07.209120Z","iopub.execute_input":"2024-04-23T04:41:07.209477Z","iopub.status.idle":"2024-04-23T04:41:07.219440Z","shell.execute_reply.started":"2024-04-23T04:41:07.209448Z","shell.execute_reply":"2024-04-23T04:41:07.218403Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                 Summary                                               Text\n0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n2  \"Delight\" says it all  This is a confection that has been around a fe...\n3         Cough Medicine  If you are looking for the secret ingredient i...\n4            Great taffy  Great taffy at a great price.  There was a wid...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"Delight\" says it all</td>\n      <td>This is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport spacy\nimport string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom concurrent.futures import ProcessPoolExecutor\nimport multiprocessing\n\n# Load the English language model in spaCy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Load the dataset\ndata = pd.read_csv('amazon_text_summary_subset.csv')  # Assuming you've saved the data subset as a CSV file\n\n\n# Define a function for text preprocessing using spaCy\ndef preprocess_text(text):\n    # Check if the input is a string\n    if isinstance(text, str):\n        # Convert text to lowercase\n        text = text.lower()\n        \n        # Remove punctuation\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        \n        # Tokenize text\n        doc = nlp(text)\n        \n        # Remove stopwords and lemmatize\n        tokens = [token.lemma_ for token in doc if token.text.lower() not in STOP_WORDS]\n        \n        # Join tokens back into a string\n        preprocessed_text = ' '.join(tokens)\n        \n        return preprocessed_text\n    else:\n        # If the input is not a string, return an empty string\n        return \"\"\n\n# Define a parallel processing function\ndef parallel_preprocess(chunk):\n    processed_rows = 0\n    total_processed = 0  # Track total processed rows\n    for i, row in chunk.iterrows():\n        chunk.at[i, 'Text'] = preprocess_text(row['Text'])\n        chunk.at[i, 'Summary'] = preprocess_text(row['Summary'])\n        processed_rows += 1\n        total_processed += 1\n        if processed_rows % 2000 == 0:\n            print(f\"Processed {total_processed} rows...\")\n            processed_rows = 0  # Reset processed_rows counter\n    print(f\"Processed {total_processed} rows...\")\n    return chunk\n\n# Split the dataset into chunks\nnum_cores = multiprocessing.cpu_count()\nchunk_size = len(data) // num_cores\ndata_chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n\n# Process data chunks in parallel\nwith ProcessPoolExecutor() as executor:\n    preprocessed_chunks = list(executor.map(parallel_preprocess, data_chunks))\n\n# Concatenate preprocessed chunks into a single DataFrame\npreprocessed_data = pd.concat(preprocessed_chunks)\n\n# Save preprocessed data to a new CSV file\npreprocessed_data.to_csv('preprocessed_amazon_text_summary.csv', index=False)\n\nprint(\"Preprocessing completed!\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:41:19.039882Z","iopub.execute_input":"2024-04-23T04:41:19.040557Z","iopub.status.idle":"2024-04-23T04:42:17.001481Z","shell.execute_reply.started":"2024-04-23T04:41:19.040530Z","shell.execute_reply":"2024-04-23T04:42:17.000329Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Processed 1250 rows...\nProcessed 1250 rows...\nProcessed 1250 rows...\nProcessed 1250 rows...\nPreprocessing completed!\n","output_type":"stream"}]},{"cell_type":"code","source":"preprocessed_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:42:21.638428Z","iopub.execute_input":"2024-04-23T04:42:21.639469Z","iopub.status.idle":"2024-04-23T04:42:21.650152Z","shell.execute_reply.started":"2024-04-23T04:42:21.639431Z","shell.execute_reply":"2024-04-23T04:42:21.649189Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                 Summary                                               Text\n0  good quality dog food  buy vitality can dog food product find good qu...\n1              advertise  product arrive label jumbo salt peanutsthe pea...\n2            delight say  confection century   light pillowy citrus gela...\n3         cough medicine  look secret ingredient robitussin believe find...\n4            great taffy  great taffy great price   wide assortment yumm...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>good quality dog food</td>\n      <td>buy vitality can dog food product find good qu...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>advertise</td>\n      <td>product arrive label jumbo salt peanutsthe pea...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>delight say</td>\n      <td>confection century   light pillowy citrus gela...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cough medicine</td>\n      <td>look secret ingredient robitussin believe find...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>great taffy</td>\n      <td>great taffy great price   wide assortment yumm...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file\ndf = pd.read_csv('/kaggle/working/preprocessed_amazon_text_summary.csv')\n\n# Drop rows with empty text in the 'Summary' column\ndf = df.dropna(subset=['Summary'])\n\n# Reset index after dropping rows\ndf = df.reset_index(drop=True)\n\n# Save the modified DataFrame back to a CSV file\ndf.to_csv('modified_file.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:42:24.642655Z","iopub.execute_input":"2024-04-23T04:42:24.643510Z","iopub.status.idle":"2024-04-23T04:42:24.737015Z","shell.execute_reply.started":"2024-04-23T04:42:24.643481Z","shell.execute_reply":"2024-04-23T04:42:24.735562Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import csv\nimport random\n\ndef split_csv(input_file, output_file1, output_file2, split_ratio=0.75):\n    # Read CSV file\n    with open(input_file, 'r', newline='') as csv_file:\n        csv_reader = csv.reader(csv_file)\n        header = next(csv_reader)  # Assuming the first row is header\n        data = list(csv_reader)\n\n    # Shuffle data randomly\n    random.shuffle(data)\n\n    # Calculate split indices\n    split_index = int(len(data) * split_ratio)\n\n    # Split data\n    data1 = data[:split_index]\n    data2 = data[split_index:]\n\n    # Write to CSV files\n    with open(output_file1, 'w', newline='') as csv_file1:\n        csv_writer1 = csv.writer(csv_file1)\n        csv_writer1.writerow(header)\n        csv_writer1.writerows(data1)\n\n    with open(output_file2, 'w', newline='') as csv_file2:\n        csv_writer2 = csv.writer(csv_file2)\n        csv_writer2.writerow(header)\n        csv_writer2.writerows(data2)\n\n# Example usage:\ninput_file = '/kaggle/working/modified_file.csv'\noutput_file1 = 'train.csv'  # 75% data\noutput_file2 = 'test.csv'  # 25% data\nsplit_csv(input_file, output_file1, output_file2)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:42:28.220841Z","iopub.execute_input":"2024-04-23T04:42:28.221666Z","iopub.status.idle":"2024-04-23T04:42:28.475004Z","shell.execute_reply.started":"2024-04-23T04:42:28.221634Z","shell.execute_reply":"2024-04-23T04:42:28.474218Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:42:33.009475Z","iopub.execute_input":"2024-04-23T04:42:33.010464Z","iopub.status.idle":"2024-04-23T04:42:46.207651Z","shell.execute_reply.started":"2024-04-23T04:42:33.010430Z","shell.execute_reply":"2024-04-23T04:42:46.206659Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TextDataset, DataCollatorForLanguageModeling\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import Trainer, TrainingArguments","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:42:46.209534Z","iopub.execute_input":"2024-04-23T04:42:46.209841Z","iopub.status.idle":"2024-04-23T04:43:00.098266Z","shell.execute_reply.started":"2024-04-23T04:42:46.209812Z","shell.execute_reply":"2024-04-23T04:43:00.097439Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"2024-04-23 04:42:50.042383: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-23 04:42:50.042517: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-23 04:42:50.176016: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import csv\n\ndef csv_to_text_file(csv_file, output_file):\n    with open(output_file, 'w', encoding='utf-8') as output:\n        with open(csv_file, 'r', newline='', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            next(reader)  # Skip the header row\n            for row in reader:\n                text = row[1]\n                summary = row[0]\n                output.write(\"[T] \" + text + \"\\n[S] \" + summary + \"\\n\\n\")\n\n# Example usage:\ncsv_file = '/kaggle/working/train.csv'        # Replace 'your_file.csv' with the path to your CSV file\noutput_file = 'reviews_train.txt'   # Replace 'output_text.txt' with the desired output text file path\ncsv_to_text_file(csv_file, output_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:43:03.441821Z","iopub.execute_input":"2024-04-23T04:43:03.442542Z","iopub.status.idle":"2024-04-23T04:43:03.463648Z","shell.execute_reply.started":"2024-04-23T04:43:03.442509Z","shell.execute_reply":"2024-04-23T04:43:03.462825Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import csv\n\ndef csv_to_text_file(csv_file, output_file):\n    with open(output_file, 'w', encoding='utf-8') as output:\n        with open(csv_file, 'r', newline='', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            next(reader)  # Skip the header row\n            for row in reader:\n                text = row[1]\n\n                output.write(\"[T] \" + text  + \"\\n\\n\")\n\n# Example usage:\ncsv_file = '/kaggle/working/test.csv'        # Replace 'your_file.csv' with the path to your CSV file\noutput_file = 'reviews_test.txt'   # Replace 'output_text.txt' with the desired output text file path\ncsv_to_text_file(csv_file, output_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:43:07.185415Z","iopub.execute_input":"2024-04-23T04:43:07.186177Z","iopub.status.idle":"2024-04-23T04:43:07.198169Z","shell.execute_reply.started":"2024-04-23T04:43:07.186144Z","shell.execute_reply":"2024-04-23T04:43:07.197141Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def load_dataset(file_path, tokenizer, block_size=128):\n    dataset = TextDataset(\n        tokenizer=tokenizer,\n        file_path=file_path,\n        block_size=block_size,\n    )\n    return dataset\n\n\ndef load_data_collator(tokenizer, mlm=False):\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=mlm,\n    )\n    return data_collator\n\n\ndef train(train_file_path, model_name,\n          output_dir,\n          overwrite_output_dir,\n          per_device_train_batch_size,\n          num_train_epochs,\n          save_steps):\n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    train_dataset = load_dataset(train_file_path, tokenizer)\n    data_collator = load_data_collator(tokenizer)\n\n    tokenizer.save_pretrained(output_dir)\n\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    model.save_pretrained(output_dir)\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=overwrite_output_dir,\n        per_device_train_batch_size=per_device_train_batch_size,\n        num_train_epochs=num_train_epochs,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n    )\n\n    trainer.train()\n    trainer.save_model()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:43:10.466731Z","iopub.execute_input":"2024-04-23T04:43:10.467221Z","iopub.status.idle":"2024-04-23T04:43:10.478719Z","shell.execute_reply.started":"2024-04-23T04:43:10.467180Z","shell.execute_reply":"2024-04-23T04:43:10.477617Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_file_path = \"/kaggle/working/reviews_train.txt\"\nmodel_name = 'gpt2'\noutput_dir = '/kaggle/working/result'\noverwrite_output_dir = False\nper_device_train_batch_size = 8\nnum_train_epochs = 15\nsave_steps = 500","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:43:26.600887Z","iopub.execute_input":"2024-04-23T04:43:26.601701Z","iopub.status.idle":"2024-04-23T04:43:26.606257Z","shell.execute_reply.started":"2024-04-23T04:43:26.601669Z","shell.execute_reply":"2024-04-23T04:43:26.605334Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train(\n    train_file_path=train_file_path,\n    model_name=model_name,\n    output_dir=output_dir,\n    overwrite_output_dir=overwrite_output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    num_train_epochs=num_train_epochs,\n    save_steps=save_steps\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:43:29.364928Z","iopub.execute_input":"2024-04-23T04:43:29.366016Z","iopub.status.idle":"2024-04-23T04:53:13.590431Z","shell.execute_reply.started":"2024-04-23T04:43:29.365954Z","shell.execute_reply":"2024-04-23T04:53:13.589349Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab5d3033fe0244d1a7d19f20dbf504e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cea7a3d402a4ac6a7435840ce306cf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5902f9245dc4191a5a651b34507cf01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a92970ff39cb49bbb7cbaf475afb0a33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68b4a2afc82246caa133d8944e08e2a5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b69e5b9640c4c4daf5c99bb3f3e7334"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ecc6649d48d47cfbe9856f5de604fe4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240423_044414-gw0g0mxr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mahisha23121/huggingface/runs/gw0g0mxr' target=\"_blank\">stoic-star-5</a></strong> to <a href='https://wandb.ai/mahisha23121/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mahisha23121/huggingface' target=\"_blank\">https://wandb.ai/mahisha23121/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mahisha23121/huggingface/runs/gw0g0mxr' target=\"_blank\">https://wandb.ai/mahisha23121/huggingface/runs/gw0g0mxr</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3075' max='3075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3075/3075 08:39, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>4.636300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>4.194700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.924600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>3.747300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>3.602500</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>3.517100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:53:16.459464Z","iopub.execute_input":"2024-04-23T04:53:16.460206Z","iopub.status.idle":"2024-04-23T04:53:16.468298Z","shell.execute_reply.started":"2024-04-23T04:53:16.460172Z","shell.execute_reply":"2024-04-23T04:53:16.466999Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def load_model(model_path):\n    model = GPT2LMHeadModel.from_pretrained(model_path)\n    return model\n\n\ndef load_tokenizer(tokenizer_path):\n    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n    return tokenizer\n\n\ndef generate_text(sequence, max_length):\n    model_path = \"/kaggle/working/result\"\n    model = load_model(model_path)\n    tokenizer = load_tokenizer(model_path)\n    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n    final_outputs = model.generate(\n        ids,\n        do_sample=True,\n        max_length=max_length,\n        pad_token_id=model.config.eos_token_id,\n        top_k=50,\n        top_p=0.95,\n    )\n    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:53:20.677036Z","iopub.execute_input":"2024-04-23T04:53:20.677885Z","iopub.status.idle":"2024-04-23T04:53:20.686577Z","shell.execute_reply.started":"2024-04-23T04:53:20.677857Z","shell.execute_reply":"2024-04-23T04:53:20.685239Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"sequence = input() # oil price\nmax_len = int(input()) # 20\ngenerate_text(sequence, max_len)","metadata":{"trusted":true},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdin","text":" try different brand hot cocoa dark milk grove square milk chocolate kcup far good   big critic family give thumb   recommend enjoy hot cocoa purchase brand\n 100\n"},{"name":"stdout","text":"try different brand hot cocoa dark milk grove square milk chocolate kcup far good   big critic family give thumb   recommend enjoy hot cocoa purchase brand kcup\n[S] great kcup\n\n[T] love popchip like taste crunchy flavor fresh little crunchy lot flavor small bag buy bag\n[S] popchip\n\n[T] purchase item online like item online grocery store try product good   little pricey not know ill order againbr br product taste great \n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T05:01:34.308128Z","iopub.execute_input":"2024-04-23T05:01:34.308500Z","iopub.status.idle":"2024-04-23T05:01:47.051788Z","shell.execute_reply.started":"2024-04-23T05:01:34.308472Z","shell.execute_reply":"2024-04-23T05:01:47.050544Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge\n\n# Example generated and reference texts\ngenerated_text = \"great kcup\"\nreference_text = \"hot cocoa kcup\" \n\n# Initialize ROUGE\nrouge = Rouge()\n\n# Calculate ROUGE scores\nscores = rouge.get_scores(generated_text, reference_text)\n\n# Print ROUGE scores\nprint(scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T05:04:22.110643Z","iopub.execute_input":"2024-04-23T05:04:22.111346Z","iopub.status.idle":"2024-04-23T05:04:22.120127Z","shell.execute_reply.started":"2024-04-23T05:04:22.111313Z","shell.execute_reply":"2024-04-23T05:04:22.118905Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[{'rouge-1': {'r': 0.3333333333333333, 'p': 0.5, 'f': 0.39999999520000007}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.3333333333333333, 'p': 0.5, 'f': 0.39999999520000007}}]\n","output_type":"stream"}]}]}